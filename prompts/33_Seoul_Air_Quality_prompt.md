# ğŸ“Œ **ì„œìš¸ì‹œ ëŒ€ê¸°ì§ˆ ë°ì´í„°ì˜ ê¸°ë³¸ í†µê³„ ë° ì‹œê°í™” ì œê³µ í”„ë¡¬í”„íŠ¸**  

ì´ ë¬¸ì„œëŠ” **GPTë¥¼ í™œìš©í•˜ì—¬ ì„œìš¸ì‹œ ëŒ€ê¸°ì§ˆ ë°ì´í„°ì˜ ê¸°ë³¸ í†µê³„ ë° ì‹œê°í™” ì œê³µ ì½”ë“œë¥¼ ìë™ ìƒì„±**í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.  
ì•„ë˜ **í”„ë¡¬í”„íŠ¸ë¥¼ GPTì— ì…ë ¥**í•˜ë©´,ì„œìš¸ì‹œ ëŒ€ê¸°ì§ˆ ë°ì´í„°ì˜ ê¸°ë³¸ í†µê³„ ë° ì‹œê°í™” ì œê³µ ì‹œê³„ì—´ ë¶„ì„ ì½”ë“œë¥¼ ì‰½ê²Œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸš€  

---

## ğŸ› ï¸ **ì†ŒìŠ¤ë§Œë“¤ê¸°**  
```plaintext
Pythonì—ì„œ TensorFlow/Kerasë¥¼ ì‚¬ìš©í•˜ì—¬ Transformer Encoder ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì¤˜.  
í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” pandas, numpy, matplotlib, tensorflow.keras ë“±ì´ í¬í•¨ë˜ì–´ì•¼ í•´.

ë‹¤ìŒ í”„ë¡œê·¸ë¨ì„ í˜¹ì‹œ ë§Œë“¤ì–´ ì¤„ìˆ˜ ìˆë‹ˆ? ì´ ì•±ì€ ì„œìš¸ì‹œ ëŒ€ê¸°ì§ˆ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. ğŸŒŸ ì£¼ìš” ê¸°ëŠ¥ ë°ì´í„° íƒìƒ‰: ì„œìš¸ì‹œ ëŒ€ê¸°ì§ˆ ë°ì´í„°ì˜ ê¸°ë³¸ í†µê³„ ë° ì‹œê°í™” ì œê³µ ì‹œê³„ì—´ ë¶„í•´: ì¶”ì„¸(Trend), ê³„ì ˆì„±(Seasonality), ë¶ˆê·œì¹™ì„±(Irregularity) ë¶„ì„ ëª¨ë¸ ë¹„êµ: ARIMA/SARIMA, ì§€ìˆ˜í‰í™œë²•, Prophet, LSTM ë“± ë‹¤ì–‘í•œ ì˜ˆì¸¡ ëª¨ë¸ ì§€ì› ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€: RMSE, MAE, RÂ² ë“± ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ ê¸°ë°˜ í‰ê°€ ğŸ› ï¸ ì‚¬ìš© ë°©ë²• ì‚¬ì´ë“œë°”ì—ì„œ ë°ì´í„° ì—…ë¡œë“œ ë˜ëŠ” API ìˆ˜ì§‘ ì˜µì…˜ ì„ íƒ ë¶„ì„í•  ì¸¡ì •ì†Œì™€ ë³€ìˆ˜(PM10, PM2.5 ë“±) ì„ íƒ ì‹œê³„ì—´ ë¶„ì„ ì˜µì…˜ ì„¤ì • í›„ ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ ê²°ê³¼ íƒ­ì—ì„œ ë‹¤ì–‘í•œ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ ë° ë¶„ì„
```
```python
# -*- coding: utf-8 -*-
"""
ì„œìš¸ì‹œ ëŒ€ê¸°ì§ˆ ì‹œê³„ì—´ ë¶„ì„/ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ (Streamlit)

ğŸŒŸ ì£¼ìš” ê¸°ëŠ¥
- ë°ì´í„° ìˆ˜ì§‘: CSV ì—…ë¡œë“œ ë˜ëŠ” ì„œìš¸ì—´ë¦°ë°ì´í„°ê´‘ì¥ API (ì‚¬ìš©ì í‚¤ í•„ìš”)
- ë°ì´í„° íƒìƒ‰: ê¸°ë³¸ í†µê³„/ê²°ì¸¡ì¹˜/ì‹œê°í™”
- ì‹œê³„ì—´ ë¶„í•´: Trend/Seasonal/Residual
- ëª¨ë¸ ë¹„êµ: ARIMA/SARIMA, Exponential Smoothing, Prophet, LSTM
- ì„±ëŠ¥ í‰ê°€: MAE, RMSE, RÂ²

ì‹¤í–‰ ë°©ë²•
$ pip install streamlit pandas numpy matplotlib plotly scikit-learn statsmodels pmdarima prophet yfinance tensorflow
$ streamlit run seoul_air_quality_timeseries_app.py

ì£¼ì˜
- Prophet ì„¤ì¹˜ê°€ OS/í™˜ê²½ì— ë”°ë¼ "prophet" ë˜ëŠ” "fbprophet" íŒ¨í‚¤ì§€ëª…ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì„œìš¸ì‹œ API ì‚¬ìš© ì‹œ, ë³¸ íŒŒì¼ ìƒë‹¨ì˜ API_KEY ê°’ì„ ì‚¬ìš©ì í‚¤ë¡œ ë³€ê²½í•˜ì„¸ìš”.


import os
import io
import time
import json
import math
import warnings
from typing import Tuple, Dict, Optional

import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import MinMaxScaler

warnings.filterwarnings("ignore")

# =============================
# ê¸€ë¡œë²Œ ì„¤ì •
# =============================
st.set_page_config(
    page_title="ì„œìš¸ì‹œ ëŒ€ê¸°ì§ˆ ì‹œê³„ì—´ ë¶„ì„",
    page_icon="ğŸŒ«ï¸",
    layout="wide",
)

API_KEY = os.getenv("SEOUL_OPEN_API_KEY", "PUT_YOUR_API_KEY_HERE")
API_ENDPOINT = "http://openapi.seoul.go.kr:8088/{key}/json/TimeAverageAirQuality/{start}/{end}/{date}/{station}"

# =============================
# ìœ í‹¸ í•¨ìˆ˜
# =============================
@st.cache_data(show_spinner=False)
def read_csv(content: bytes, encoding: str = "utf-8") -> pd.DataFrame:
    try:
        return pd.read_csv(io.BytesIO(content), encoding=encoding)
    except Exception:
        return pd.read_csv(io.BytesIO(content), encoding="cp949")


def safe_to_datetime(s, utc=False):
    x = pd.to_datetime(s, errors="coerce")
    if utc:
        try:
            x = x.dt.tz_localize(None)
        except Exception:
            pass
    return x


def train_test_split_series(y: pd.Series, test_size: int) -> Tuple[pd.Series, pd.Series]:
    if test_size <= 0:
        return y, pd.Series(dtype=float)
    return y.iloc[:-test_size], y.iloc[-test_size:]


def metrics_dict(y_true, y_pred) -> Dict[str, float]:
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = float(np.sqrt(mse))
    r2 = r2_score(y_true, y_pred)
    return {"MAE": mae, "RMSE": rmse, "R2": r2}


def plot_actual_vs_pred(df_true: pd.DataFrame, df_pred: pd.DataFrame, date_col: str, y_col: str, model_name: str):
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=df_true[date_col], y=df_true[y_col], name="Actual", mode="lines"))
    fig.add_trace(go.Scatter(x=df_pred[date_col], y=df_pred["yhat"], name=f"Predicted - {model_name}", mode="lines"))
    fig.update_layout(title=f"Actual vs Predicted ({model_name})", xaxis_title="Date", yaxis_title=y_col, height=420)
    st.plotly_chart(fig, use_container_width=True)


# =============================
# ë°ì´í„° ìˆ˜ì§‘
# =============================
@st.cache_data(show_spinner=True)
def fetch_seoul_air_quality(station: str, start_date: str, end_date: str, pollutant: str) -> pd.DataFrame:
    """
    ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ ì˜ˆì‹œ Endpointë¥¼ ê°€ì •í•©ë‹ˆë‹¤. ì‹¤ì œ ìš´ì˜ê³¼ ìŠ¤í‚¤ë§ˆëŠ” ì‚¬ìš©ì í‚¤/ë°ì´í„°ì…‹ì— ë§ì¶° ì¡°ì • í•„ìš”.
    ë°˜í™˜ ì»¬ëŸ¼ ì˜ˆì‹œ: [date, station, PM10, PM25, O3, NO2, CO, SO2]
    """
    # ë³¸ APIëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤. ë°ì´í„°ì…‹ë§ˆë‹¤ ìŠ¤í‚¤ë§ˆê°€ ë‹¤ë¥´ë¯€ë¡œ ì‚¬ìš©ì ë§ì¶¤ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.
    # ì•ˆì „í•˜ê²ŒëŠ” ì—…ë¡œë“œ CSVë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.
    try:
        import requests
        rows = []
        # ë‹¨ìˆœ ì˜ˆì‹œ: í•˜ë£¨ ë‹¨ìœ„ ë£¨í”„ í˜¸ì¶œ (ì‹¤ì„œë²„ ê³¼ê¸ˆ/ì¿¼í„° ì£¼ì˜)
        dates = pd.date_range(start=start_date, end=end_date, freq="D")
        for d in dates:
            url = API_ENDPOINT.format(
                key=API_KEY,
                start=1,
                end=1000,
                date=d.strftime("%Y%m%d"),
                station=station,
            )
            r = requests.get(url, timeout=10)
            if r.status_code != 200:
                continue
            js = r.json()
            # ì‚¬ìš©ìì˜ ì‹¤ì œ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ íŒŒì‹± ë¡œì§ ë³€ê²½ í•„ìš”
            # ì—¬ê¸°ì„œëŠ” ê°€ìƒì˜ JSON êµ¬ì¡°ë¥¼ ê°€ì •í•©ë‹ˆë‹¤.
            data = js.get("TimeAverageAirQuality", {}).get("row", [])
            for item in data:
                rows.append(item)
        df = pd.DataFrame(rows)
        # í‘œì¤€í™” ì˜ˆì‹œ
        if not df.empty:
            # ì»¬ëŸ¼ ì¡´ì¬ ê°€ì •
            rename_map = {
                "MSRDT": "date",
                "MSRSTE_NM": "station",
                "PM10": "PM10",
                "PM25": "PM2.5",
                "O3": "O3",
                "NO2": "NO2",
                "CO": "CO",
                "SO2": "SO2",
            }
            df = df.rename(columns=rename_map)
            df["date"] = safe_to_datetime(df["date"])  # YYYY-MM-DD HH:MM
            df = df.sort_values("date").dropna(subset=[pollutant])
        return df
    except Exception as e:
        st.warning(f"API ìˆ˜ì§‘ ì‹¤íŒ¨: {e}\nCSV ì—…ë¡œë“œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
        return pd.DataFrame()


# =============================
# ì‹œê³„ì—´ ë¶„í•´
# =============================

def decompose_series(df: pd.DataFrame, date_col: str, y_col: str, model: str = "additive", period: Optional[int] = None):
    from statsmodels.tsa.seasonal import seasonal_decompose
    s = df.set_index(date_col)[y_col].asfreq("D")  # ì¼ë‹¨ìœ„ ë¹ˆë„ ê°€ì •
    s = s.interpolate(limit_direction="both")
    result = seasonal_decompose(s, model=model, period=period)
    figs = []
    for name, comp in zip(["Observed", "Trend", "Seasonal", "Residual"], [s, result.trend, result.seasonal, result.resid]):
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=comp.index, y=comp.values, mode="lines", name=name))
        fig.update_layout(title=name, height=300)
        figs.append(fig)
    return figs


# =============================
# ëª¨ë¸: ARIMA/SARIMA
# =============================

def fit_arima(df: pd.DataFrame, date_col: str, y_col: str, seasonal: bool, m: int, test_size: int):
    try:
        import pmdarima as pm
    except Exception:
        st.error("pmdarimaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install pmdarima í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        return None

    y = df[y_col].astype(float).values
    train, test = y[:-test_size], y[-test_size:]

    if seasonal:
        model = pm.auto_arima(
            train,
            seasonal=True,
            m=m,
            stepwise=True,
            suppress_warnings=True,
            error_action="ignore",
            trace=False,
        )
    else:
        model = pm.auto_arima(
            train,
            seasonal=False,
            stepwise=True,
            suppress_warnings=True,
            error_action="ignore",
            trace=False,
        )

    preds = model.predict(n_periods=test_size)
    met = metrics_dict(test, preds)
    # ì˜ˆì¸¡ ì‹œê³„ì—´ í”„ë ˆì„
    pred_df = df[[date_col]].iloc[-test_size:].copy()
    pred_df["yhat"] = preds
    return model, pred_df, met


# =============================
# ëª¨ë¸: Exponential Smoothing
# =============================

def fit_expsmoothing(df: pd.DataFrame, date_col: str, y_col: str, seasonal: bool, m: int, test_size: int):
    from statsmodels.tsa.holtwinters import ExponentialSmoothing

    y = df[y_col].astype(float).values
    train, test = y[:-test_size], y[-test_size:]

    if seasonal:
        model = ExponentialSmoothing(train, trend="add", seasonal="add", seasonal_periods=m)
    else:
        model = ExponentialSmoothing(train, trend="add", seasonal=None)

    fit = model.fit(optimized=True)
    preds = fit.forecast(test_size)
    met = metrics_dict(test, preds)
    pred_df = df[[date_col]].iloc[-test_size:].copy()
    pred_df["yhat"] = preds
    return fit, pred_df, met


# =============================
# ëª¨ë¸: Prophet
# =============================

def fit_prophet(df: pd.DataFrame, date_col: str, y_col: str, use_log: bool, test_size: int, weekly=True, yearly=True):
    # prophet import í˜¸í™˜ ì²˜ë¦¬
    try:
        from prophet import Prophet
    except Exception:
        try:
            from fbprophet import Prophet  # type: ignore
        except Exception:
            st.error("prophet/fbprophetê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install prophet í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            return None

    d = df[[date_col, y_col]].rename(columns={date_col: "ds", y_col: "y"}).copy()
    if use_log:
        d["y"] = np.log1p(d["y"].astype(float))

    train, test = d.iloc[:-test_size].copy(), d.iloc[-test_size:].copy()

    m = Prophet(weekly_seasonality=weekly, yearly_seasonality=yearly, daily_seasonality=False, changepoint_prior_scale=0.1)
    m.add_country_holidays(country_name="KR")
    m.fit(train)

    f = test[["ds"]].copy()
    fc = m.predict(f)

    y_true = test["y"].values
    y_pred = fc["yhat"].values
    if use_log:
        y_true = np.expm1(y_true)
        y_pred = np.expm1(y_pred)

    met = metrics_dict(y_true, y_pred)
    pred_df = pd.DataFrame({"ds": test["ds"], "yhat": y_pred})
    pred_df = pred_df.rename(columns={"ds": date_col})
    return m, pred_df, met


# =============================
# ëª¨ë¸: LSTM (ê°„ë‹¨ êµ¬í˜„)
# =============================

def make_sliding_window(arr: np.ndarray, look_back: int) -> Tuple[np.ndarray, np.ndarray]:
    X, y = [], []
    for i in range(len(arr) - look_back):
        X.append(arr[i : i + look_back])
        y.append(arr[i + look_back])
    X = np.array(X)
    y = np.array(y)
    return X, y


def fit_lstm(df: pd.DataFrame, date_col: str, y_col: str, look_back: int, epochs: int, batch: int, test_size: int):
    try:
        import tensorflow as tf
        from tensorflow.keras import layers, models
    except Exception:
        st.error("TensorFlowê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install tensorflow í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        return None

    values = df[y_col].astype(float).values.reshape(-1, 1)
    scaler = MinMaxScaler()
    scaled = scaler.fit_transform(values)

    X, y = make_sliding_window(scaled.flatten(), look_back)
    X = X.reshape((X.shape[0], X.shape[1], 1))

    X_train, X_test = X[:-test_size], X[-test_size:]
    y_train, y_test = y[:-test_size], y[-test_size:]

    model = models.Sequential([
        layers.Input(shape=(look_back, 1)),
        layers.LSTM(64, return_sequences=True),
        layers.Dropout(0.2),
        layers.LSTM(32),
        layers.Dense(1)
    ])
    model.compile(optimizer="adam", loss="mse")
    with st.spinner("LSTM í•™ìŠµ ì¤‘..."):
        model.fit(X_train, y_train, epochs=epochs, batch_size=batch, verbose=0, validation_data=(X_test, y_test))

    preds_scaled = model.predict(X_test, verbose=0).flatten()
    preds = scaler.inverse_transform(preds_scaled.reshape(-1, 1)).flatten()
    y_true = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()

    met = metrics_dict(y_true, preds)
    pred_df = df[[date_col]].iloc[-test_size:].copy()
    pred_df["yhat"] = preds
    return model, pred_df, met


# =============================
# ì•± UI
# =============================

def main():
    st.title("ğŸŒ«ï¸ ì„œìš¸ì‹œ ëŒ€ê¸°ì§ˆ ì‹œê³„ì—´ ë¶„ì„ & ì˜ˆì¸¡")
    st.caption("CSV ì—…ë¡œë“œ ë˜ëŠ” API ìˆ˜ì§‘ í›„, ë‹¤ì–‘í•œ ëª¨ë¸ë¡œ ì˜ˆì¸¡í•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•˜ì„¸ìš”.")

    with st.sidebar:
        st.header("ë°ì´í„° ì…ë ¥")
        src = st.radio("ë°ì´í„° ì†ŒìŠ¤", ["CSV ì—…ë¡œë“œ", "ì„œìš¸ì‹œ API"], horizontal=True)

        default_pollutants = ["PM10", "PM2.5", "O3", "NO2", "CO", "SO2"]

        if src == "CSV ì—…ë¡œë“œ":
            file = st.file_uploader("CSV íŒŒì¼ ì—…ë¡œë“œ", type=["csv"])
            date_col = st.text_input("ë‚ ì§œ ì»¬ëŸ¼ëª…", value="date")
            station_col = st.text_input("ì¸¡ì •ì†Œ ì»¬ëŸ¼ëª…", value="station")
            pollutant = st.selectbox("ë³€ìˆ˜ ì„ íƒ", options=default_pollutants, index=0)
        else:
            st.info("API ì‚¬ìš© ì‹œ, í™˜ê²½ë³€ìˆ˜ SEOUL_OPEN_API_KEY ë˜ëŠ” ì½”ë“œ ìƒë‹¨ API_KEYì— í‚¤ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.")
            station = st.text_input("ì¸¡ì •ì†Œëª… (ì˜ˆ: ì¢…ë¡œêµ¬)", value="ì¢…ë¡œêµ¬")
            start_date = st.date_input("ì‹œì‘ì¼", value=pd.to_datetime("2024-01-01").date())
            end_date = st.date_input("ì¢…ë£Œì¼", value=pd.to_datetime("today").date())
            pollutant = st.selectbox("ë³€ìˆ˜ ì„ íƒ", options=default_pollutants, index=0)

        st.divider()
        st.header("ì‹œê³„ì—´ ì„¤ì •")
        test_size = st.number_input("í…ŒìŠ¤íŠ¸ ê¸°ê°„(í¬ì¸íŠ¸ ìˆ˜)", min_value=7, max_value=180, value=30, step=1)
        seasonal = st.checkbox("ê³„ì ˆì„± ì‚¬ìš© (ARIMA/ES)", value=True)
        m = st.number_input("ê³„ì ˆ ì£¼ê¸° m (ì¼ ë‹¨ìœ„)", min_value=1, max_value=365, value=7)

        st.divider()
        st.header("ëª¨ë¸ ì„ íƒ")
        use_arima = st.checkbox("ARIMA/SARIMA", value=True)
        use_es = st.checkbox("Exponential Smoothing", value=True)
        use_prophet = st.checkbox("Prophet", value=True)
        use_lstm = st.checkbox("LSTM", value=False)

        if use_prophet:
            use_log = st.checkbox("Prophet ë¡œê·¸ë³€í™˜", value=True)
        else:
            use_log = False

        if use_lstm:
            look_back = st.number_input("LSTM look_back", min_value=5, max_value=60, value=20)
            epochs = st.number_input("LSTM epochs", min_value=5, max_value=200, value=50)
            batch = st.number_input("LSTM batch_size", min_value=8, max_value=256, value=32)
        else:
            look_back = epochs = batch = None

    # ë°ì´í„° ë¡œë”©
    df = pd.DataFrame()
    if src == "CSV ì—…ë¡œë“œ" and file is not None:
        df = read_csv(file.getvalue())
        # í‘œì¤€ ì»¬ëŸ¼ëª… ì²´í¬
        if date_col not in df.columns:
            st.error(f"CSVì— '{date_col}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        if pollutant not in df.columns:
            st.error(f"CSVì— '{pollutant}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        if station_col not in df.columns:
            st.warning(f"CSVì— '{station_col}' ì»¬ëŸ¼ì´ ì—†ì–´ ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            df[station_col] = "UNKNOWN"
        df = df[[date_col, station_col, pollutant]].copy()
        df[date_col] = safe_to_datetime(df[date_col], utc=True)
        df = df.dropna(subset=[date_col, pollutant]).sort_values(date_col)
    elif src == "ì„œìš¸ì‹œ API":
        df = fetch_seoul_air_quality(
            station=station,
            start_date=str(start_date),
            end_date=str(end_date),
            pollutant=pollutant,
        )
        if df.empty:
            st.stop()
        date_col = "date"
        station_col = "station"
    else:
        st.info("ì‚¬ì´ë“œë°”ì—ì„œ CSVë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ API ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”.")
        st.stop()

    st.subheader("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
    st.dataframe(df.head(20), use_container_width=True)

    st.subheader("ê¸°ë³¸ í†µê³„")
    c1, c2, c3 = st.columns(3)
    with c1:
        st.metric("ê¸°ê°„", f"{df[date_col].min().date()} ~ {df[date_col].max().date()}")
    with c2:
        st.metric("ê´€ì¸¡ì¹˜ ìˆ˜", f"{len(df):,}")
    with c3:
        st.metric("ê²°ì¸¡ì¹˜ ìˆ˜", f"{df[pollutant].isna().sum():,}")

    st.subheader("ì‹œê°í™”")
    line_fig = px.line(df, x=date_col, y=pollutant, color=station_col if station_col in df.columns else None, title=f"{pollutant} ì‹œê³„ì—´")
    st.plotly_chart(line_fig, use_container_width=True)

    with st.expander("íˆìŠ¤í† ê·¸ë¨ / ìƒìê·¸ë¦¼"):
        c1, c2 = st.columns(2)
        with c1:
            st.plotly_chart(px.histogram(df, x=pollutant, nbins=40, title="ë¶„í¬"), use_container_width=True)
        with c2:
            st.plotly_chart(px.box(df, y=pollutant, points="suspectedoutliers", title="ìƒìê·¸ë¦¼"), use_container_width=True)

    # íŠ¹ì • ì¸¡ì •ì†Œ í•„í„° ì˜µì…˜ (CSVì— station_col ìˆëŠ” ê²½ìš°)
    if station_col in df.columns:
        stations = sorted(df[station_col].dropna().unique().tolist())
        sel_station = st.selectbox("ë¶„ì„í•  ì¸¡ì •ì†Œ", options=stations)
        dfx = df[df[station_col] == sel_station].copy()
    else:
        sel_station = "ALL"
        dfx = df.copy()

    dfx = dfx.dropna(subset=[pollutant]).sort_values(date_col)

    # ì‹œê³„ì—´ ë¶„í•´
    st.subheader("ì‹œê³„ì—´ ë¶„í•´")
    try:
        figs = decompose_series(dfx[[date_col, pollutant]], date_col, pollutant, model="additive", period=m if seasonal else None)
        for fig in figs:
            st.plotly_chart(fig, use_container_width=True)
    except Exception as e:
        st.warning(f"ì‹œê³„ì—´ ë¶„í•´ ì‹¤íŒ¨: {e}")

    # ëª¨ë¸ í•™ìŠµ & ë¹„êµ
    st.subheader("ëª¨ë¸ í•™ìŠµ ë° ë¹„êµ")
    if len(dfx) <= test_size + 10:
        st.error("ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ êµ¬ê°„ + 10 ì´ìƒì˜ ê´€ì¸¡ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        st.stop()

    results = []  # (name, pred_df, metrics)

    # ê³µí†µ í‘œì¤€í™” í”„ë ˆì„
    std_df = dfx[[date_col, pollutant]].rename(columns={pollutant: "y"}).copy()

    # ARIMA/SARIMA
    if use_arima:
        with st.spinner("ARIMA/SARIMA í•™ìŠµ ì¤‘..."):
            out = fit_arima(std_df, date_col, "y", seasonal=seasonal, m=int(m), test_size=int(test_size))
        if out is not None:
            model, pred_df, met = out
            results.append(("ARIMA/SARIMA", pred_df, met))
            plot_actual_vs_pred(std_df.iloc[-test_size:], pred_df, date_col, "y", "ARIMA/SARIMA")

    # Exponential Smoothing
    if use_es:
        with st.spinner("Exponential Smoothing í•™ìŠµ ì¤‘..."):
            out = fit_expsmoothing(std_df, date_col, "y", seasonal=seasonal, m=int(m), test_size=int(test_size))
        if out is not None:
            model, pred_df, met = out
            results.append(("ExpSmoothing", pred_df, met))
            plot_actual_vs_pred(std_df.iloc[-test_size:], pred_df, date_col, "y", "ExpSmoothing")

    # Prophet
    if use_prophet:
        with st.spinner("Prophet í•™ìŠµ ì¤‘..."):
            out = fit_prophet(std_df, date_col, "y", use_log=use_log, test_size=int(test_size))
        if out is not None:
            model, pred_df, met = out
            results.append(("Prophet", pred_df, met))
            plot_actual_vs_pred(std_df.iloc[-test_size:], pred_df, date_col, "y", "Prophet")

    # LSTM
    if use_lstm:
        with st.spinner("LSTM í•™ìŠµ ì¤‘..."):
            out = fit_lstm(std_df, date_col, "y", look_back=int(look_back), epochs=int(epochs), batch=int(batch), test_size=int(test_size))
        if out is not None:
            model, pred_df, met = out
            results.append(("LSTM", pred_df, met))
            plot_actual_vs_pred(std_df.iloc[-test_size:], pred_df, date_col, "y", "LSTM")

    # ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”
    if results:
        st.subheader("ì„±ëŠ¥ ë¹„êµ (í…ŒìŠ¤íŠ¸ êµ¬ê°„)")
        rows = []
        for name, _, met in results:
            row = {"Model": name}
            row.update(met)
            rows.append(row)
        score_df = pd.DataFrame(rows).sort_values("RMSE")
        st.dataframe(score_df, use_container_width=True)

        # ì¢…í•© ë¹„êµ í”Œë¡¯ (ê²¹ì³ ë³´ê¸°)
        st.subheader("ì˜ˆì¸¡ ê²°ê³¼ ê²¹ì³ ë³´ê¸°")
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=std_df.iloc[-test_size:][date_col], y=std_df.iloc[-test_size:]["y"], name="Actual", mode="lines"))
        for name, pred_df, _ in results:
            fig.add_trace(go.Scatter(x=pred_df[date_col], y=pred_df["yhat"], name=name, mode="lines"))
        fig.update_layout(title="Test Window Forecast Comparison", xaxis_title="Date", yaxis_title=pollutant, height=500)
        st.plotly_chart(fig, use_container_width=True)

    st.success("ì™„ë£Œ! ì‚¬ì´ë“œë°” ì˜µì…˜ì„ ë³€ê²½í•´ ë‹¤ë¥¸ ì¡°í•©ë„ ì‹¤í—˜í•´ ë³´ì„¸ìš”.")


if __name__ == "__main__":
    main()

```